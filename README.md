# TemporalAttentionPlayground

### Evaluating Temporal Context for Robustness to Perturbations in Video Object Detection Models

This repository contains code and experiments for the bachelor thesis "Evaluating Temporal Context for Robustness to Perturbations in Video Object Detection Models". It is exploring temporal attention mechanisms for object detection in video sequences using UAV (Unmanned Aerial Vehicle) datasets **VisDrone** and **XS-VID**. The project evaluates the performance of temporal attention models **TRANSVOD** and **YOLOV**  as well as there baselines **MSDA** and **YOLOX**.

The repository supports:
- ğŸ”§ Enviroment setup
- ğŸ“¦ Dataset structure generation & preprocessing
- ğŸ”¥ Model setup and training
- ğŸ“Š Evaluation and result visualization
- â™»ï¸ FAIR and reproducible experiment design

---

## ğŸš€ Project Overview

| Component | Description |
|-----------|-------------|
| ğŸ¯ **Goal** | Investigate influence of temporal attention on complex datasets and study the influence of temporal attention on model robustness |
| ğŸ§ª **Datasets** |VisDrone (drone-based object detection)  XS-VID (small object dataset)|
| ğŸ§  **Framework** | CV Deep learning (PyTorch-based, CUDA)|
| ğŸ’¡ **Output** | Thesis, Evaluation metrics (mAP, AP), trained models, prediction results, examples |
| â™»ï¸ **FAIR compliance** | Code stored in Git, data in RDM repository (DOI forthcoming), fully documented |

---

## ğŸ“‚ Repository Structure

```text
.
â”œâ”€ code/                 # Python code for setup, dataset building
â”œâ”€ setup/                # Setup sript and environment information
â”œâ”€ data/                 # Data folder with dataset information and example
â”œâ”€ results/              # Summary metrics & figures (full outputs in RDM repository)
â”œâ”€ README.md             # This file
â”œâ”€ scripts/              # Easy to use scripts for training, evaluating and result generation
â””â”€ .gitignore
```

## ğŸ”§ Setup

### ğŸ“Œ Preconditions

Before running the setup, ensure that you have:

- NVIDIA GPU supporting **CUDA 11.3**
- At least **50 GB** of free storage space
- A **Linux-based system** (recommended) with `sudo` permissions
- **Conda** or **Python 3.8+** installed

---

### ğŸš€ Quick Setup Instructions
1. Clone repository including submodules
    ```bash
    git clone --recurse-submodules https://github.com/mozi30/TemporalAttentionPlayground.git
    ```

2. Navigate to the setup directory:

   ```bash
   cd setup
   ```
3. Configure required paths by editing the config.env file:
    ```bash
   nano setup.env
   ```
   Adjust the values according to your system, for example:
    1. Base environment path
    2. Dataset storage location.
    3. Output directory for annotations
    4. Directory for model weights
3. Run the setup script:
    ```
    sudo bash setup-env.sh
    ````
    This script will:
        Install required environments and dependencies (e.g. msda, YOLOX)
    - Download datasets from the specified storage location
    - Generate annotations in the correct format for training and evaluation
    - Download base model weights
    - Finalize the environment for model training and evaluation

You are ready to start working ğŸš€ğŸš€ğŸš€

For more information on training and evaluation check out `scripts/README.md`

## ğŸ“š Reproducibility & FAIR Principles

This project is designed according to the FAIR principles (Findable, Accessible, Interoperable, Reusable). The experiment follows the data lifecycle described in the Data Management Plan (DMP).

### ğŸ” Findable
- Code is version-controlled in this Git repository.
- Full experimental results, dataset annotations, and trained model weights will be published in the TU Wien Research Data Repository (**DOI: _to be added_**).
- Each dataset used (VisDrone, XS-VID) is referenced with its official source and citation.
Further information in `data/README.md`

### ğŸ”“ Accessible
- Code and lightweight experiment results are openly available in this repository.
- Full datasets and large result files (e.g., video sequences, large model checkpoints) are available via RDM repository access, according to their respective licenses.
- Repository includes clear instructions on how to obtain and prepare input data.

### ğŸ” Interoperable
- Standard formats are used whenever possible (JSON, YAML, COCO-style annotations, PNG/JPEG images).
- Data processing follows structured Python pipelines.
- Configuration files (e.g. `.env`, YAML) allow reproducibility of the setup.

### ğŸ”‚ Reusable
- Code will be provided under the **MIT License** (see `LICENSE` file).
- Produced experimental data will be shared under **CC-BY 4.0** unless restricted by dataset terms.
- Detailed metadata is provided in:
  - `data/README.md` â€“ dataset structure and label mapping
  - `results/README.md` â€“ explanation of metrics and result files
  - `scripts/` â€“ runnable experiment scripts
  - DMP (deposited to Zenodo, DOI pending)

---

## ğŸ“ Data Management Plan (DMP)

A detailed DMP following **Science Europe Guidelines** has been created for this project. It includes:

- Data sources and licensing
- Data processing workflow and reproducibility
- Storage, backup, and access strategy
- Metadata and documentation standards
- FAIR self-assessment and steps taken

The DMP will be deposited on **Zenodo** as part of the _Intro RDM â€“ DMPs 2025_ collection (embargoed until deadline).

ğŸ”¹ **DMP Title**: `DMP: Evaluating Temporal Context for Robustness to Perturbations in Video Object Detection Models`
ğŸ”¹ **DOI:** _to be added_ (once uploaded to Zenodo)

---

## ğŸ“œ Licensing

| Component     | License            |
|---------------|-------------------|
| **Code**      | MIT License        |
| **Produced Data & Results** | CC BY-NC-SA 3.0 for results including Visdrone, MIT for XS-VID |
| **Input Data** | Per dataset terms (VisDrone: CC BY-NC-SA 3.0, XS-VID: MIT) |

#### Check dataset license before redistribution.

The appropriate license files will be added to the repository and confirmed in the DMP.

---
